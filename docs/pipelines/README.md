# ğŸ”„ Pipelines de Dados - MMarra Data Hub

**VersÃ£o:** 1.0.0
**Data:** 2026-02-03
**Status:** âœ… Operacional

---

## ğŸ“‹ VisÃ£o Geral

Pipelines sÃ£o fluxos automatizados de dados que movem informaÃ§Ãµes do ERP Sankhya para o Azure Data Lake, seguindo a arquitetura **Medallion** (Bronze/Silver/Gold).

### Arquitetura de Dados

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      SANKHYA ERP                             â”‚
â”‚                    (Fonte de Dados)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PIPELINE DE EXTRAÃ‡ÃƒO                        â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ Clientes â”‚    â”‚ Produtos â”‚    â”‚ Estoque  â”‚    ...       â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜              â”‚
â”‚       â”‚               â”‚               â”‚                     â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                       â–¼                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   AZURE DATA LAKE                            â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  RAW (Bronze)                                        â”‚   â”‚
â”‚  â”‚  - Dados brutos do Sankhya                          â”‚   â”‚
â”‚  â”‚  - Formato: Parquet                                 â”‚   â”‚
â”‚  â”‚  - Sem transformaÃ§Ã£o                                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â”‚                                  â”‚
â”‚                          â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  PROCESSED (Silver) [Futuro]                        â”‚   â”‚
â”‚  â”‚  - Dados limpos e validados                         â”‚   â”‚
â”‚  â”‚  - DeduplicaÃ§Ã£o                                     â”‚   â”‚
â”‚  â”‚  - Tipagem correta                                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â”‚                                  â”‚
â”‚                          â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  CURATED (Gold) [Futuro]                            â”‚   â”‚
â”‚  â”‚  - Dados agregados                                  â”‚   â”‚
â”‚  â”‚  - MÃ©tricas calculadas                              â”‚   â”‚
â”‚  â”‚  - Prontos para BI                                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Pipelines DisponÃ­veis

### 1. Pipeline de ExtraÃ§Ã£o

**Arquivo:** `src/pipelines/extracao.py`

**FunÃ§Ã£o:** Extrair dados do Sankhya e carregar no Data Lake (camada RAW).

**Entidades extraÃ­das:**

| Entidade | Registros | Tamanho | FrequÃªncia sugerida |
|----------|-----------|---------|---------------------|
| Vendedores | ~111 | 0.01 MB | Semanal |
| Clientes | ~57.000 | 4.02 MB | DiÃ¡rio |
| Produtos | ~393.000 | 9.67 MB | DiÃ¡rio |
| Estoque | ~19.000 | 0.46 MB | HorÃ¡rio |
| **TOTAL** | **~470.000** | **~14 MB** | - |

**CaracterÃ­sticas:**
- Contorna limite de 5000 registros da API usando faixas
- Sobrescreve arquivos anteriores (sem versionamento)
- Salva cÃ³pia local + upload para Azure

---

## ğŸš€ Como Usar

### ExecuÃ§Ã£o Completa

```bash
cd mmarra-data-hub
python src/pipelines/extracao.py
```

### Via CÃ³digo Python

```python
from src.pipelines import PipelineExtracao

# Criar pipeline
pipeline = PipelineExtracao()

# Executar todas as entidades
resultados = pipeline.executar()

# Ou executar entidades especÃ­ficas
resultados = pipeline.executar(entidades=["clientes", "estoque"])
```

### Via Script Dedicado

```bash
# Extrair tudo
python scripts/extrair_tudo.py

# Extrair entidade especÃ­fica
python scripts/extrair_para_datalake.py --extrator clientes
```

---

## âš™ï¸ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

```bash
# mcp_sankhya/.env

# Sankhya API
SANKHYA_CLIENT_ID=...
SANKHYA_CLIENT_SECRET=...
SANKHYA_X_TOKEN=...

# Azure Data Lake
AZURE_STORAGE_ACCOUNT=mmarradatalake
AZURE_STORAGE_KEY=...
AZURE_CONTAINER=datahub
```

### ConfiguraÃ§Ã£o de Entidades

Cada entidade pode ser configurada em `PipelineExtracao.ENTIDADES`:

```python
"clientes": {
    "query_template": "SELECT ... WHERE {WHERE_FAIXA}",
    "colunas": ["CODPARC", "NOMEPARC", ...],
    "caminho_datalake": "raw/clientes/clientes.parquet",
    "usa_faixa": True,
    "campo_id": "p.CODPARC",
    "id_max": 100000,
    "faixa_size": 5000
}
```

---

## ğŸ“ Estrutura no Data Lake

```
datahub/
â”œâ”€â”€ raw/                          # Camada Bronze
â”‚   â”œâ”€â”€ vendedores/
â”‚   â”‚   â””â”€â”€ vendedores.parquet
â”‚   â”œâ”€â”€ clientes/
â”‚   â”‚   â””â”€â”€ clientes.parquet
â”‚   â”œâ”€â”€ produtos/
â”‚   â”‚   â””â”€â”€ produtos.parquet
â”‚   â”œâ”€â”€ estoque/
â”‚   â”‚   â””â”€â”€ estoque.parquet
â”‚   â””â”€â”€ vendas/                   # [Futuro]
â”‚       â””â”€â”€ vendas.parquet
â”‚
â”œâ”€â”€ processed/                    # Camada Silver [Futuro]
â”‚   â”œâ”€â”€ dim_clientes/
â”‚   â”œâ”€â”€ dim_produtos/
â”‚   â””â”€â”€ fact_vendas/
â”‚
â””â”€â”€ curated/                      # Camada Gold [Futuro]
    â”œâ”€â”€ metricas_vendas/
    â”œâ”€â”€ metricas_estoque/
    â””â”€â”€ dashboards/
```

---

## ğŸ“Š Monitoramento

### Resultado da ExecuÃ§Ã£o

Cada execuÃ§Ã£o retorna uma lista de resultados:

```python
[
    {
        "entidade": "clientes",
        "registros": 57082,
        "sucesso": True,
        "tamanho_mb": 4.02
    },
    ...
]
```

### Logs

Os pipelines usam `logging` do Python:

```
2026-02-03 11:00:00 - INFO - Autenticando no Sankhya...
2026-02-03 11:00:01 - INFO - Autenticado com sucesso
2026-02-03 11:00:02 - INFO - Conectando ao Azure Data Lake...
2026-02-03 11:00:03 - INFO - clientes: 57082 registros (4.02 MB)
2026-02-03 11:00:04 - INFO - clientes: Upload concluÃ­do
```

---

## ğŸ”® Roadmap

### Implementado
- [x] Pipeline de extraÃ§Ã£o (RAW)
- [x] ExtraÃ§Ã£o por faixas (contorna limite API)
- [x] Upload para Azure Data Lake

### PrÃ³ximos Passos
- [ ] Pipeline de transformaÃ§Ã£o (PROCESSED)
  - Limpeza de dados
  - DeduplicaÃ§Ã£o
  - ValidaÃ§Ã£o de tipos
- [ ] Pipeline de agregaÃ§Ã£o (CURATED)
  - MÃ©tricas de vendas
  - Indicadores de estoque
- [ ] Agendamento automÃ¡tico
  - Azure Functions
  - Cron job
- [ ] Alertas de falha
  - Email
  - Teams/Slack
- [ ] ExtraÃ§Ã£o incremental
  - Apenas registros alterados
  - Baseado em DTALTER

---

## ğŸ› ï¸ Troubleshooting

### Erro: Timeout na API

**Problema:** Query demora mais de 180 segundos.

**SoluÃ§Ã£o:** Reduzir `faixa_size` para 2000 ou 1000.

### Erro: Limite de 5000 registros

**Problema:** API retorna apenas 5000 registros.

**SoluÃ§Ã£o:** JÃ¡ contornado com extraÃ§Ã£o por faixas. Verificar se `usa_faixa=True`.

### Erro: Upload falhou

**Problema:** Falha ao enviar para Azure.

**SoluÃ§Ã£o:**
1. Verificar credenciais no `.env`
2. Verificar conexÃ£o com internet
3. Verificar permissÃµes no container

---

## ğŸ“š ReferÃªncias

- [Azure Data Lake Documentation](https://docs.microsoft.com/azure/storage/blobs/data-lake-storage-introduction)
- [Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture)
- [DocumentaÃ§Ã£o Sankhya API](docs/api/sankhya.md)
